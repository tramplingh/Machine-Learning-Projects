# -*- coding: utf-8 -*-
"""FloodMapDL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11AYI1bgq3vW78PpsNvBRapnXZl4v9RtY
"""

!pip install torch torchvision segmentation-models-pytorch albumentations opencv-python matplotlib tqdm -q

from google.colab import drive
drive.mount('/content/drive/')

import os, random, shutil
from glob import glob

IMG_DIR  = "/content/drive/MyDrive/FloodMappingData/Images"
MASK_DIR = "/content/drive/MyDrive/FloodMappingData/Masks"
OUT_DIR  = "/content/data_split"

for split in ["train","val","test"]:
    for sub in ["images","masks"]:
        os.makedirs(os.path.join(OUT_DIR, split, sub), exist_ok=True)

images = sorted([os.path.basename(p) for p in glob(os.path.join(IMG_DIR, "*.png"))])
paired = [f for f in images if os.path.exists(os.path.join(MASK_DIR, f))]

random.seed(42)
random.shuffle(paired)
n = len(paired)
n_train, n_val = int(0.8*n), int(0.1*n)
splits = {
    "train": paired[:n_train],
    "val": paired[n_train:n_train+n_val],
    "test": paired[n_train+n_val:]
}

for split, files in splits.items():
    for f in files:
        shutil.copy(os.path.join(IMG_DIR, f),  os.path.join(OUT_DIR, split, "images", f))
        shutil.copy(os.path.join(MASK_DIR, f), os.path.join(OUT_DIR, split, "masks", f))

print({k: len(v) for k,v in splits.items()})

import cv2, torch, numpy as np, albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.utils.data import Dataset, DataLoader
import os

IMG_SIZE = 256

train_tfms = A.Compose([
    A.Resize(IMG_SIZE,IMG_SIZE),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
    ToTensorV2()
])
val_tfms = A.Compose([
    A.Resize(IMG_SIZE,IMG_SIZE),
    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
    ToTensorV2()
])

class SegDataset(Dataset):
    def __init__(self,img_dir,mask_dir,tfm=None):
        self.img_dir, self.mask_dir, self.tfm = img_dir, mask_dir, tfm
        self.imgs = []
        for f in sorted(os.listdir(img_dir)):
            img_path = os.path.join(img_dir, f)
            mask_path = os.path.join(mask_dir, f)
            # Check if both image and mask files exist and can be read
            if os.path.exists(mask_path):
                try:
                    img = cv2.imread(img_path)
                    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                    if img is not None and mask is not None:
                        self.imgs.append(f)
                    else:
                        print(f"Warning: Skipping {f} due to read error.")
                except Exception as e:
                    print(f"Warning: Skipping {f} due to error: {e}")


    def __len__(self): return len(self.imgs)
    def __getitem__(self,idx):
        img = cv2.cvtColor(cv2.imread(os.path.join(self.img_dir,self.imgs[idx])),cv2.COLOR_BGR2RGB)
        mask= cv2.imread(os.path.join(self.mask_dir,self.imgs[idx]),cv2.IMREAD_GRAYSCALE)
        mask = (mask>127).astype("float32")
        if self.tfm:
            aug=self.tfm(image=img,mask=mask); img,mask=aug["image"],aug["mask"]
        return img, mask.unsqueeze(0)

BASE="/content/data_split"
train_ds=SegDataset(f"{BASE}/train/images",f"{BASE}/train/masks",train_tfms)
val_ds  =SegDataset(f"{BASE}/val/images",  f"{BASE}/val/masks",  val_tfms)
test_ds =SegDataset(f"{BASE}/test/images", f"{BASE}/test/masks", val_tfms)

train_dl=DataLoader(train_ds,batch_size=8,shuffle=True,num_workers=2,pin_memory=True)
val_dl  =DataLoader(val_ds,batch_size=8,shuffle=False,num_workers=2,pin_memory=True)

import segmentation_models_pytorch as smp
import torch.nn as nn

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:",device)

model=smp.Unet(encoder_name="resnet34",encoder_weights="imagenet",
               in_channels=3,classes=1).to(device)

bce = nn.BCEWithLogitsLoss()
dice= smp.losses.DiceLoss(mode='binary')
def loss_fn(pred,y): return bce(pred,y)+dice(pred,y)

opt=torch.optim.Adam(model.parameters(),lr=1e-4)

from tqdm import tqdm
import torch

#Intersection over Union
def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor, threshold=0.5):
    preds = (torch.sigmoid(outputs) > threshold).int()
    labels = labels.int()

    intersection = (preds & labels).float().sum((1, 2))
    union = (preds | labels).float().sum((1, 2))

    iou = (intersection + 1e-6) / (union + 1e-6)

    return iou.mean()


# Training and validation loop
best_val_iou = 0.0
num_epochs = 10

# Lists to store metrics for plotting
train_losses = []
val_losses = []
val_ious = []

for epoch in range(num_epochs):
    #Training
    model.train()
    total_train_loss = 0.0

    train_progress_bar = tqdm(train_dl, desc=f"Epoch {epoch+1}/{num_epochs} [Training]")
    for x, y in train_progress_bar:
        x, y = x.to(device), y.to(device)

        opt.zero_grad()
        pred = model(x)
        loss = loss_fn(pred, y)
        loss.backward()
        opt.step()

        total_train_loss += loss.item()
        # progress bar
        train_progress_bar.set_postfix(loss=total_train_loss / len(train_progress_bar))

    avg_train_loss = total_train_loss / len(train_dl)
    train_losses.append(avg_train_loss)

    #Validation
    model.eval()
    total_val_loss = 0.0
    total_val_iou = 0.0

    val_progress_bar = tqdm(val_dl, desc=f"Epoch {epoch+1}/{num_epochs} [Validation]")
    with torch.no_grad():
        for x, y in val_progress_bar:
            x, y = x.to(device), y.to(device)

            pred = model(x)
            loss = loss_fn(pred, y)
            iou = iou_pytorch(pred, y)

            total_val_loss += loss.item()
            total_val_iou += iou.item()

            # progress bar
            val_progress_bar.set_postfix(
                val_loss=total_val_loss / len(val_progress_bar),
                val_iou=total_val_iou / len(val_progress_bar)
            )

    avg_val_loss = total_val_loss / len(val_dl)
    avg_val_iou = total_val_iou / len(val_dl)
    val_losses.append(avg_val_loss)
    val_ious.append(avg_val_iou)

    print(f"Epoch {epoch+1}/{num_epochs} -> Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val IoU: {avg_val_iou:.4f}")

    # Save the model if it has the best validation IoU so far
    if avg_val_iou > best_val_iou:
        best_val_iou = avg_val_iou
        torch.save(model.state_dict(), "best_unet.pth")
        print(f"Model Saved! New best validation IoU: {best_val_iou:.4f}")

print("\n--- Training Complete! ---")

import matplotlib.pyplot as plt

# Plotting loss
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plotting IoU
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs + 1), val_ious, label='Validation IoU')
plt.xlabel('Epoch')
plt.ylabel('IoU')
plt.title('Validation IoU over Epochs')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt, glob, cv2, numpy as np
model.load_state_dict(torch.load("best_unet.pth",map_location=device))
model.eval()

def show_pred(img_path,thr=0.5,alpha=0.5):
    img=cv2.cvtColor(cv2.imread(img_path),cv2.COLOR_BGR2RGB)
    h,w=img.shape[:2]
    aug=val_tfms(image=img)
    x=aug["image"].unsqueeze(0).to(device)
    with torch.no_grad():
        pred=torch.sigmoid(model(x))[0][0].cpu().numpy()
    pred_resized=cv2.resize(pred,(w,h))
    heatmap=(pred_resized*255).astype(np.uint8)
    heatmap=cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)
    heatmap=cv2.cvtColor(heatmap,cv2.COLOR_BGR2RGB)
    overlay=cv2.addWeighted(img,1-alpha,heatmap,alpha,0)
    plt.figure(figsize=(12,4))
    plt.subplot(1,3,1);plt.imshow(img);plt.title("Original");plt.axis('off')
    plt.subplot(1,3,2);plt.imshow(pred_resized,cmap='jet');plt.title("Heatmap");plt.axis('off')
    plt.subplot(1,3,3);plt.imshow(overlay);plt.title("Overlay");plt.axis('off')
    plt.show()

sample=sorted(glob.glob(f"{BASE}/test/images/*"))[:5]
for s in sample: show_pred(s)

import torch
from tqdm import tqdm

model.load_state_dict(torch.load("best_unet.pth", map_location=device))
model.eval()

test_dl = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)

total_test_iou = 0.0
test_progress_bar = tqdm(test_dl, desc="Evaluating on Test Set")

with torch.no_grad():
    for x, y in test_progress_bar:
        x, y = x.to(device), y.to(device)
        pred = model(x)
       iou = iou_pytorch(pred, y)
        total_test_iou += iou.item()

avg_test_iou = total_test_iou / len(test_dl)

print("\n--- Testing Complete ---")
print(f"Final Average IoU on the Test Set: {avg_test_iou:.4f}")

!pip install streamlit -q

# Commented out IPython magic to ensure Python compatibility.
# # app.py
# %%writefile app.py
# import streamlit as st
# import torch
# import cv2
# import numpy as np
# import albumentations as A
# from albumentations.pytorch import ToTensorV2
# import segmentation_models_pytorch as smp
# 
# # --- CONFIGURATION ---
# st.set_page_config(layout="wide", page_title="Flood Mapping AI")
# st.title("ðŸŒŠ AI-Powered Flood Mapping Tool")
# st.write("Upload a satellite or aerial image to detect and visualize flooded areas.")
# 
# # --- MODEL AND TRANSFORMS ---
# DEVICE = torch.device("cpu") # Use CPU for deployment
# IMG_SIZE = 256
# VAL_TFMS = A.Compose([
#     A.Resize(IMG_SIZE, IMG_SIZE),
#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
#     ToTensorV2()
# ])
# 
# @st.cache_resource # Cache the model for faster re-runs
# def load_model():
#     model = smp.Unet(encoder_name="resnet34", in_channels=3, classes=1)
#     model.load_state_dict(torch.load("best_unet.pth", map_location=DEVICE))
#     model.eval()
#     return model
# 
# model = load_model()
# 
# # --- PREDICTION AND VISUALIZATION FUNCTION ---
# def predict_and_visualize(image_bytes, alpha=0.5):
#     # Load image
#     nparr = np.frombuffer(image_bytes, np.uint8)
#     img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#     h, w = img.shape[:2]
# 
#     # Preprocess and predict
#     aug = VAL_TFMS(image=img)
#     x = aug["image"].unsqueeze(0).to(DEVICE)
#     with torch.no_grad():
#         pred = torch.sigmoid(model(x))[0][0].cpu().numpy()
# 
#     # Resize prediction to original image size
#     pred_resized = cv2.resize(pred, (w, h))
# 
#     # Create heatmap
#     heatmap = (pred_resized * 255).astype(np.uint8)
#     heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
#     heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)
# 
#     # Create overlay
#     overlay = cv2.addWeighted(img, 1 - alpha, heatmap_color, alpha, 0)
# 
#     return img, heatmap_color, overlay
# 
# # --- STREAMLIT UI ---
# uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
# overlay_alpha = st.slider("Overlay Transparency", 0.0, 1.0, 0.5, 0.05)
# 
# if uploaded_file is not None:
#     st.info("Processing image... Please wait.")
#     original, heatmap, overlay = predict_and_visualize(uploaded_file.getvalue(), overlay_alpha)
# 
#     st.success("Processing complete!")
#     col1, col2, col3 = st.columns(3)
#     with col1:
#         st.image(original, caption="Original Image", use_column_width=True)
#     with col2:
#         st.image(heatmap, caption="Flood Heatmap", use_column_width=True)
#     with col3:
#         st.image(overlay, caption="Overlay", use_column_width=True)

!pip freeze > requirements.txt

!streamlit run app.py



"""## AI-Powered Flood Mapping Project

This project demonstrates how to build a deep learning model for semantic segmentation to identify flooded areas in satellite or aerial imagery.

**Technologies Used:**

*   **Python:** The primary programming language for data processing, model training, and application development.
*   **PyTorch:** A deep learning framework used to build, train, and evaluate the Unet model.
*   **Segmentation Models PyTorch:** A library providing pre-trained segmentation models like U-Net.
*   **Albumentations:** A library for image augmentation, used to enhance the training dataset.
*   **OpenCV:** Used for image loading, processing, and manipulation.
*   **NumPy:** For numerical operations and array handling.
*   **Matplotlib:** For visualizing training metrics and model predictions.
*   **tqdm:** For displaying progress bars during training and evaluation.
*   **Streamlit:** A framework used to create a simple web application for demonstrating the flood mapping model.
*   **Google Colab:** The platform used for developing and running the notebook, providing access to GPUs for faster training.
*   **Google Drive:** Used to store and access the image and mask datasets.

**Real-Life Use Cases:**

*   **Disaster Response and Management:** Quickly identify flooded areas after heavy rainfall or natural disasters to assess the impact, prioritize rescue efforts, and allocate resources effectively.
*   **Urban Planning:** Analyze historical flood data and predict potential flood zones to inform urban development, infrastructure planning, and land-use regulations.
*   **Environmental Monitoring:** Track changes in water bodies and identify areas prone to flooding due to climate change or other environmental factors.
*   **Insurance and Risk Assessment:** Help insurance companies assess flood risk for properties and inform policy decisions.
*   **Agriculture:** Identify flooded agricultural land to estimate crop damage and plan for recovery.

**Next Steps:**

*   **Improve Model Performance:** Experiment with different encoder architectures, hyperparameters, and loss functions to further improve the model's accuracy.
*   **Expand Dataset:** Train the model on a larger and more diverse dataset to improve its generalization capabilities.
*   **Implement More Sophisticated Post-processing:** Explore advanced techniques for refining the model's output, such as conditional random fields (CRFs).
*   **Deploy as a Web Service:** Deploy the trained model as a robust web service for wider accessibility and integration into other applications.
*   **Real-time Processing:** Investigate methods for near real-time flood detection using streaming data.
*   **Explore Different Data Sources:** Incorporate other data sources like satellite radar data or social media reports to enhance flood detection.
"""